name: Model Retraining

on:
  schedule:
    # Run every Sunday at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      force_retrain:
        description: 'Force model retraining'
        required: false
        default: false
        type: boolean

jobs:
  check-model-performance:
    runs-on: ubuntu-latest
    outputs:
      should_retrain: ${{ steps.check.outputs.retrain }}
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e .
    
    - name: Check model performance
      id: check
      run: |
        # Generate fresh data for evaluation
        python scripts/generate_data.py
        
        # Evaluate current model performance
        python -m src.models.evaluate
        
        # Check if retraining is needed (placeholder logic)
        # In practice, you'd compare metrics against thresholds
        if [ "${{ github.event.inputs.force_retrain }}" = "true" ]; then
          echo "retrain=true" >> $GITHUB_OUTPUT
        else
          # Add logic to check performance degradation
          echo "retrain=false" >> $GITHUB_OUTPUT
        fi

  retrain-model:
    runs-on: ubuntu-latest
    needs: check-model-performance
    if: needs.check-model-performance.outputs.should_retrain == 'true'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e .
    
    - name: Create directories
      run: |
        mkdir -p data/raw data/processed data/features data/external
        mkdir -p models/artifacts models/experiments models/registry
        mkdir -p logs metrics
    
    - name: Generate fresh training data
      run: |
        python scripts/generate_data.py
    
    - name: Retrain model
      run: |
        python -m src.models.train --config config/model_config.yaml
    
    - name: Evaluate retrained model
      run: |
        python -m src.models.evaluate
    
    - name: Upload retrained model artifacts
      uses: actions/upload-artifact@v4
      with:
        name: retrained-model-${{ github.run_number }}
        path: |
          models/
          mlruns/
    
    - name: Create Pull Request with new model
      uses: peter-evans/create-pull-request@v5
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        commit-message: "feat: retrained model with improved performance"
        title: "ðŸ¤– Automated Model Retraining - Run #${{ github.run_number }}"
        body: |
          ## ðŸ¤– Automated Model Retraining
          
          This PR contains a retrained model with updated performance metrics.
          
          **Training Details:**
          - Triggered: ${{ github.event_name }}
          - Run ID: ${{ github.run_number }}
          - Timestamp: ${{ github.event.head_commit.timestamp }}
          
          **Next Steps:**
          1. Review model performance metrics
          2. Test the retrained model
          3. Approve and merge if performance is satisfactory
          
          The retrained model artifacts are available in the workflow run.
        branch: automated-retrain-${{ github.run_number }}
        delete-branch: true
