name: Performance Monitoring

on:
  schedule:
    # Run performance tests daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:

jobs:
  api-performance-test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Setup project
      run: make setup
    
    - name: Install performance testing tools
      run: pip install locust
    
    - name: Generate test data
      run: make generate-data
    
    - name: Train model for testing
      run: make train-model
    
    - name: Start API server
      run: |
        python -m uvicorn src.api.app:app --host 0.0.0.0 --port 8000 &
        sleep 10
    
    - name: Run performance tests
      run: |
        # Create a simple locust test file
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        import json
        import random

        class APIUser(HttpUser):
            wait_time = between(1, 3)
            
            @task(3)
            def health_check(self):
                self.client.get("/health")
            
            @task(2)
            def single_prediction(self):
                payload = {
                    "customer_id": f"CUST_{random.randint(1000, 9999)}",
                    "tenure": random.randint(1, 72),
                    "monthly_charges": round(random.uniform(20, 120), 2),
                    "total_charges": round(random.uniform(100, 8000), 2),
                    "contract": random.choice(["Month-to-month", "One year", "Two year"]),
                    "payment_method": random.choice(["Electronic check", "Mailed check", "Bank transfer", "Credit card"]),
                    "internet_service": random.choice(["DSL", "Fiber optic", "No"]),
                    "online_security": random.choice(["Yes", "No"]),
                    "tech_support": random.choice(["Yes", "No"]),
                    "streaming_tv": random.choice(["Yes", "No"])
                }
                self.client.post("/predict", json=payload)
            
            @task(1)
            def metrics(self):
                self.client.get("/metrics/performance")
        EOF
        
        # Run load test
        locust -f locustfile.py --host=http://localhost:8000 --users 10 --spawn-rate 2 --run-time 60s --html performance-report.html --csv performance-results
    
    - name: Upload performance reports
      uses: actions/upload-artifact@v4
      with:
        name: performance-reports
        path: |
          performance-report.html
          performance-results_*.csv

  model-drift-detection:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Setup project
      run: make setup
    
    - name: Generate baseline and new data
      run: |
        # Generate baseline data
        make generate-data
        mv data/raw/customer_churn.csv data/raw/baseline_data.csv
        
        # Generate new data with potential drift
        make generate-data
        mv data/raw/customer_churn.csv data/raw/new_data.csv
    
    - name: Run drift detection
      run: |
        python -c "
        import pandas as pd
        from scipy import stats
        import numpy as np
        
        # Load data
        baseline = pd.read_csv('data/raw/baseline_data.csv')
        new_data = pd.read_csv('data/raw/new_data.csv')
        
        # Simple drift detection using KS test
        numerical_cols = ['tenure', 'monthly_charges', 'total_charges']
        drift_detected = False
        
        for col in numerical_cols:
            ks_stat, p_value = stats.ks_2samp(baseline[col], new_data[col])
            print(f'{col}: KS statistic = {ks_stat:.4f}, p-value = {p_value:.4f}')
            if p_value < 0.05:
                print(f'DRIFT DETECTED in {col}!')
                drift_detected = True
        
        if drift_detected:
            print('⚠️ Data drift detected! Consider model retraining.')
            exit(1)
        else:
            print('✅ No significant drift detected.')
        "
    
    - name: Create drift alert issue
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: '⚠️ Data Drift Detected - Model Retraining Required',
            body: `## Data Drift Alert
            
            Automated drift detection has identified significant changes in the data distribution.
            
            **Details:**
            - Detection Date: ${new Date().toISOString()}
            - Workflow Run: ${context.runId}
            
            **Recommended Actions:**
            1. Review the drift detection results
            2. Analyze the new data patterns
            3. Consider triggering model retraining
            4. Update monitoring thresholds if needed
            
            **Workflow:** [View Run](${context.payload.repository.html_url}/actions/runs/${context.runId})
            `,
            labels: ['drift-alert', 'model-ops', 'high-priority']
          })
