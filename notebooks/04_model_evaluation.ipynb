{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation - Customer Churn Prediction\n",
    "\n",
    "This notebook provides comprehensive evaluation of the trained churn prediction model including performance metrics, error analysis, and business impact assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score, accuracy_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data and trained model\n",
    "print(\"=== LOADING DATA AND MODEL ===\")\n",
    "\n",
    "try:\n",
    "    # Load test data\n",
    "    X_test = pd.read_csv('../data/processed/X_test.csv')\n",
    "    y_test = pd.read_csv('../data/processed/y_test.csv').squeeze()\n",
    "    \n",
    "    # Load model metadata\n",
    "    with open('../models/artifacts/model_metadata.json', 'r') as f:\n",
    "        model_metadata = json.load(f)\n",
    "    \n",
    "    # Load the trained model\n",
    "    model_name = model_metadata['model_name'].lower().replace(' ', '_')\n",
    "    model_path = f'../models/artifacts/best_churn_model_{model_name}.joblib'\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    print(f\"âœ… Loaded model: {model_metadata['model_name']}\")\n",
    "    print(f\"âœ… Test data shape: {X_test.shape}\")\n",
    "    print(f\"âœ… Model type: {model_metadata['model_type']}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ Error loading files: {e}\")\n",
    "    print(\"Please run the previous notebooks first:\")\n",
    "    print(\"1. 02_feature_engineering.ipynb\")\n",
    "    print(\"2. 03_model_training.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "print(\"=== GENERATING PREDICTIONS ===\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Predictions generated for {len(y_test)} samples\")\n",
    "print(f\"Predicted churn rate: {y_pred.mean():.2%}\")\n",
    "print(f\"Actual churn rate: {y_test.mean():.2%}\")\n",
    "print(f\"Prediction probability range: {y_pred_proba.min():.3f} - {y_pred_proba.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive performance metrics\n",
    "print(\"=== PERFORMANCE METRICS ===\")\n",
    "\n",
    "# Basic metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "# Create metrics dataframe\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'Avg Precision'],\n",
    "    'Score': [accuracy, precision, recall, f1, roc_auc, avg_precision]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(metrics_df.round(4))\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Analysis\n",
    "print(\"=== CONFUSION MATRIX ANALYSIS ===\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "\n",
    "# Calculate rates\n",
    "specificity = tn / (tn + fp)\n",
    "sensitivity = tp / (tp + fn)  # Same as recall\n",
    "false_positive_rate = fp / (fp + tn)\n",
    "false_negative_rate = fn / (fn + tp)\n",
    "\n",
    "print(f\"\\nSpecificity (True Negative Rate): {specificity:.4f}\")\n",
    "print(f\"Sensitivity (True Positive Rate): {sensitivity:.4f}\")\n",
    "print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
    "print(f\"False Negative Rate: {false_negative_rate:.4f}\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Churn', 'Churn'], \n",
    "            yticklabels=['No Churn', 'Churn'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and Precision-Recall Curves\n",
    "print(\"=== ROC AND PRECISION-RECALL CURVES ===\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n",
    "axes[0].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "axes[1].plot(recall_curve, precision_curve, label=f'PR Curve (AP = {avg_precision:.3f})')\n",
    "axes[1].axhline(y=y_test.mean(), color='k', linestyle='--', label='Baseline')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = roc_thresholds[optimal_idx]\n",
    "print(f\"\\nOptimal threshold (Youden's J): {optimal_threshold:.3f}\")\n",
    "print(f\"At optimal threshold - TPR: {tpr[optimal_idx]:.3f}, FPR: {fpr[optimal_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold Analysis\n",
    "print(\"=== THRESHOLD ANALYSIS ===\")\n",
    "\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "threshold_metrics = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred_thresh)\n",
    "    prec = precision_score(y_test, y_pred_thresh)\n",
    "    rec = recall_score(y_test, y_pred_thresh)\n",
    "    f1_thresh = f1_score(y_test, y_pred_thresh)\n",
    "    \n",
    "    threshold_metrics.append({\n",
    "        'Threshold': threshold,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1_thresh\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_metrics)\n",
    "print(\"Metrics by Threshold:\")\n",
    "print(threshold_df.round(3))\n",
    "\n",
    "# Plot threshold analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['Accuracy'], 'o-', label='Accuracy')\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['Precision'], 's-', label='Precision')\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['Recall'], '^-', label='Recall')\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['F1-Score'], 'd-', label='F1-Score')\n",
    "plt.axvline(x=0.5, color='k', linestyle='--', alpha=0.5, label='Default (0.5)')\n",
    "plt.axvline(x=optimal_threshold, color='r', linestyle='--', alpha=0.7, label=f'Optimal ({optimal_threshold:.2f})')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance by Classification Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration Analysis\n",
    "print(\"=== CALIBRATION ANALYSIS ===\")\n",
    "\n",
    "# Calibration curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Model\")\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Probability\")\n",
    "plt.ylabel(\"Fraction of Positives\")\n",
    "plt.title(\"Calibration Plot\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Prediction distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_pred_proba[y_test == 0], bins=20, alpha=0.7, label='No Churn', density=True)\n",
    "plt.hist(y_pred_proba[y_test == 1], bins=20, alpha=0.7, label='Churn', density=True)\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Prediction Probability Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calibration metrics\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "brier_score = brier_score_loss(y_test, y_pred_proba)\n",
    "print(f\"Brier Score (lower is better): {brier_score:.4f}\")\n",
    "print(f\"Calibration Error: {np.mean(np.abs(fraction_of_positives - mean_predicted_value)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis\n",
    "print(\"=== ERROR ANALYSIS ===\")\n",
    "\n",
    "# Create error analysis dataframe\n",
    "error_df = X_test.copy()\n",
    "error_df['actual'] = y_test\n",
    "error_df['predicted'] = y_pred\n",
    "error_df['predicted_proba'] = y_pred_proba\n",
    "error_df['correct'] = (y_test == y_pred)\n",
    "\n",
    "# False Positives (predicted churn but didn't churn)\n",
    "false_positives = error_df[(error_df['actual'] == 0) & (error_df['predicted'] == 1)]\n",
    "print(f\"False Positives: {len(false_positives)} ({len(false_positives)/len(error_df)*100:.1f}%)\")\n",
    "\n",
    "# False Negatives (predicted no churn but did churn)\n",
    "false_negatives = error_df[(error_df['actual'] == 1) & (error_df['predicted'] == 0)]\n",
    "print(f\"False Negatives: {len(false_negatives)} ({len(false_negatives)/len(error_df)*100:.1f}%)\")\n",
    "\n",
    "# Analyze characteristics of errors\n",
    "if len(false_positives) > 0 and len(false_negatives) > 0:\n",
    "    print(\"\\nCharacteristics of False Positives (avg):\")\n",
    "    numerical_cols = error_df.select_dtypes(include=[np.number]).columns\n",
    "    fp_stats = false_positives[numerical_cols].mean()\n",
    "    overall_stats = error_df[error_df['actual'] == 0][numerical_cols].mean()\n",
    "    \n",
    "    comparison_fp = pd.DataFrame({\n",
    "        'False_Positives': fp_stats,\n",
    "        'All_Non_Churners': overall_stats,\n",
    "        'Difference': fp_stats - overall_stats\n",
    "    }).round(3)\n",
    "    \n",
    "    print(comparison_fp.head(10))\n",
    "    \n",
    "    print(\"\\nCharacteristics of False Negatives (avg):\")\n",
    "    fn_stats = false_negatives[numerical_cols].mean()\n",
    "    overall_churn_stats = error_df[error_df['actual'] == 1][numerical_cols].mean()\n",
    "    \n",
    "    comparison_fn = pd.DataFrame({\n",
    "        'False_Negatives': fn_stats,\n",
    "        'All_Churners': overall_churn_stats,\n",
    "        'Difference': fn_stats - overall_churn_stats\n",
    "    }).round(3)\n",
    "    \n",
    "    print(comparison_fn.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Impact Analysis\n",
    "print(\"=== BUSINESS IMPACT ANALYSIS ===\")\n",
    "\n",
    "# Define business parameters (example values)\n",
    "avg_customer_value = 1200  # Average annual revenue per customer\n",
    "retention_cost = 100       # Cost to retain a customer\n",
    "acquisition_cost = 300     # Cost to acquire a new customer\n",
    "\n",
    "# Calculate business metrics\n",
    "total_customers = len(y_test)\n",
    "actual_churners = sum(y_test)\n",
    "predicted_churners = sum(y_pred)\n",
    "\n",
    "# Confusion matrix values\n",
    "true_positives = tp\n",
    "false_positives = fp\n",
    "false_negatives = fn\n",
    "true_negatives = tn\n",
    "\n",
    "# Business impact calculations\n",
    "revenue_saved = true_positives * avg_customer_value  # Correctly identified churners\n",
    "unnecessary_retention_cost = false_positives * retention_cost  # Wasted retention efforts\n",
    "lost_revenue = false_negatives * avg_customer_value  # Missed churners\n",
    "total_retention_cost = predicted_churners * retention_cost\n",
    "\n",
    "# Net business impact\n",
    "net_impact = revenue_saved - unnecessary_retention_cost - total_retention_cost\n",
    "baseline_loss = actual_churners * avg_customer_value  # Loss without any intervention\n",
    "improvement = baseline_loss - (lost_revenue + total_retention_cost)\n",
    "\n",
    "print(f\"Business Impact Analysis:\")\n",
    "print(f\"Total customers evaluated: {total_customers:,}\")\n",
    "print(f\"Actual churners: {actual_churners:,}\")\n",
    "print(f\"Predicted churners: {predicted_churners:,}\")\n",
    "print(f\"\\nFinancial Impact:\")\n",
    "print(f\"Revenue saved (TP): ${revenue_saved:,.2f}\")\n",
    "print(f\"Unnecessary retention costs (FP): ${unnecessary_retention_cost:,.2f}\")\n",
    "print(f\"Lost revenue (FN): ${lost_revenue:,.2f}\")\n",
    "print(f\"Total retention costs: ${total_retention_cost:,.2f}\")\n",
    "print(f\"\\nNet business improvement: ${improvement:,.2f}\")\n",
    "print(f\"ROI of churn prediction: {(improvement/total_retention_cost)*100:.1f}%\")\n",
    "\n",
    "# Create business impact visualization\n",
    "impact_data = {\n",
    "    'Revenue Saved\\n(True Positives)': revenue_saved,\n",
    "    'Lost Revenue\\n(False Negatives)': -lost_revenue,\n",
    "    'Wasted Costs\\n(False Positives)': -unnecessary_retention_cost,\n",
    "    'Retention Costs': -total_retention_cost\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['green', 'red', 'orange', 'blue']\n",
    "bars = plt.bar(impact_data.keys(), impact_data.values(), color=colors, alpha=0.7)\n",
    "plt.title('Business Impact Analysis')\n",
    "plt.ylabel('Financial Impact ($)')\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, impact_data.values()):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + (5000 if height > 0 else -15000),\n",
    "             f'${value:,.0f}', ha='center', va='bottom' if height > 0 else 'top')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation Summary\n",
    "print(\"=== MODEL EVALUATION SUMMARY ===\")\n",
    "\n",
    "evaluation_summary = {\n",
    "    'model_name': model_metadata['model_name'],\n",
    "    'model_type': model_metadata['model_type'],\n",
    "    'test_samples': len(y_test),\n",
    "    'actual_churn_rate': y_test.mean(),\n",
    "    'predicted_churn_rate': y_pred.mean(),\n",
    "    'performance_metrics': {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'average_precision': avg_precision,\n",
    "        'brier_score': brier_score\n",
    "    },\n",
    "    'confusion_matrix': {\n",
    "        'true_negatives': int(tn),\n",
    "        'false_positives': int(fp),\n",
    "        'false_negatives': int(fn),\n",
    "        'true_positives': int(tp)\n",
    "    },\n",
    "    'business_impact': {\n",
    "        'revenue_saved': revenue_saved,\n",
    "        'lost_revenue': lost_revenue,\n",
    "        'unnecessary_costs': unnecessary_retention_cost,\n",
    "        'total_retention_costs': total_retention_cost,\n",
    "        'net_improvement': improvement,\n",
    "        'roi_percentage': (improvement/total_retention_cost)*100\n",
    "    },\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "    'evaluation_date': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save evaluation results\n",
    "import os\n",
    "os.makedirs('../models/evaluation', exist_ok=True)\n",
    "\n",
    "with open('../models/evaluation/evaluation_results.json', 'w') as f:\n",
    "    json.dump(evaluation_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"âœ… Evaluation results saved to ../models/evaluation/evaluation_results.json\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nðŸŽ¯ FINAL EVALUATION SUMMARY\")\n",
    "print(f\"Model: {model_metadata['model_name']}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Business ROI: {(improvement/total_retention_cost)*100:.1f}%\")\n",
    "print(f\"\\nðŸŽ‰ Model evaluation completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
