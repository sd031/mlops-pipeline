{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Customer Churn Prediction\n",
    "\n",
    "This notebook focuses on feature engineering and preprocessing for the customer churn prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/raw/customer_churn.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality check\n",
    "print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "print(f\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"=== HANDLING MISSING VALUES ===\")\n",
    "\n",
    "# Check for missing values in total_charges (common issue)\n",
    "if 'total_charges' in df.columns:\n",
    "    # Convert total_charges to numeric (might be stored as string)\n",
    "    df['total_charges'] = pd.to_numeric(df['total_charges'], errors='coerce')\n",
    "    \n",
    "    # Fill missing total_charges with median\n",
    "    if df['total_charges'].isnull().sum() > 0:\n",
    "        median_charges = df['total_charges'].median()\n",
    "        df['total_charges'].fillna(median_charges, inplace=True)\n",
    "        print(f\"Filled {df['total_charges'].isnull().sum()} missing total_charges with median: {median_charges}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "initial_shape = df.shape[0]\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Removed {initial_shape - df.shape[0]} duplicate rows\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "# Create new features\n",
    "if 'total_charges' in df.columns and 'monthly_charges' in df.columns:\n",
    "    # Average monthly charges over tenure\n",
    "    df['avg_monthly_charges'] = df['total_charges'] / (df['tenure'] + 1)  # +1 to avoid division by zero\n",
    "    \n",
    "    # Charges per month ratio\n",
    "    df['charges_ratio'] = df['monthly_charges'] / df['avg_monthly_charges']\n",
    "    \n",
    "    # Total charges per year\n",
    "    df['annual_charges'] = df['monthly_charges'] * 12\n",
    "\n",
    "# Tenure categories\n",
    "if 'tenure' in df.columns:\n",
    "    df['tenure_group'] = pd.cut(df['tenure'], \n",
    "                               bins=[0, 12, 24, 48, 72], \n",
    "                               labels=['0-1 year', '1-2 years', '2-4 years', '4+ years'])\n",
    "\n",
    "# Monthly charges categories\n",
    "if 'monthly_charges' in df.columns:\n",
    "    df['charges_group'] = pd.cut(df['monthly_charges'], \n",
    "                                bins=[0, 35, 65, 95, float('inf')], \n",
    "                                labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "print(\"New features created:\")\n",
    "new_features = ['avg_monthly_charges', 'charges_ratio', 'annual_charges', 'tenure_group', 'charges_group']\n",
    "for feature in new_features:\n",
    "    if feature in df.columns:\n",
    "        print(f\"- {feature}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "print(\"=== CATEGORICAL ENCODING ===\")\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Remove target variable from categorical columns if present\n",
    "if 'churn' in categorical_cols:\n",
    "    categorical_cols.remove('churn')\n",
    "\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "\n",
    "# Create a copy for encoding\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "for col in categorical_cols:\n",
    "    if col in df_encoded.columns:\n",
    "        # Get dummies and add to dataframe\n",
    "        dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=True)\n",
    "        df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "        df_encoded.drop(col, axis=1, inplace=True)\n",
    "\n",
    "print(f\"\\nAfter encoding - Dataset shape: {df_encoded.shape}\")\n",
    "print(f\"New columns: {df_encoded.shape[1] - df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "print(\"=== FEATURE SCALING ===\")\n",
    "\n",
    "# Prepare features and target\n",
    "if 'churn' in df_encoded.columns:\n",
    "    X = df_encoded.drop('churn', axis=1)\n",
    "    y = df_encoded['churn']\n",
    "else:\n",
    "    X = df_encoded\n",
    "    y = None\n",
    "\n",
    "# Remove customer_id if present\n",
    "if 'customer_id' in X.columns:\n",
    "    X = X.drop('customer_id', axis=1)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "if y is not None:\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns\n",
    "X_scaled = X.copy()\n",
    "X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "print(f\"\\nScaled {len(numerical_features)} numerical features\")\n",
    "print(f\"Numerical features: {list(numerical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "\n",
    "if y is not None:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "    \n",
    "    # Random Forest feature importance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_scaled, y)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_scaled.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 most important features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 15 Feature Importance (Random Forest)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mutual information\n",
    "    mi_scores = mutual_info_classif(X_scaled, y, random_state=42)\n",
    "    mi_df = pd.DataFrame({\n",
    "        'feature': X_scaled.columns,\n",
    "        'mutual_info': mi_scores\n",
    "    }).sort_values('mutual_info', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 features by Mutual Information:\")\n",
    "    print(mi_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "print(\"=== TRAIN-TEST SPLIT ===\")\n",
    "\n",
    "if y is not None:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "    print(f\"\\nTraining set churn distribution:\")\n",
    "    print(y_train.value_counts(normalize=True))\n",
    "    print(f\"\\nTest set churn distribution:\")\n",
    "    print(y_test.value_counts(normalize=True))\n",
    "    \n",
    "    # Save processed data\n",
    "    import os\n",
    "    os.makedirs('../data/processed', exist_ok=True)\n",
    "    \n",
    "    # Save training and test sets\n",
    "    X_train.to_csv('../data/processed/X_train.csv', index=False)\n",
    "    X_test.to_csv('../data/processed/X_test.csv', index=False)\n",
    "    y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "    y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "    \n",
    "    # Save full processed dataset\n",
    "    processed_data = pd.concat([X_scaled, y], axis=1)\n",
    "    processed_data.to_csv('../data/processed/customer_churn_processed.csv', index=False)\n",
    "    \n",
    "    print(\"\\nâœ… Processed data saved to ../data/processed/\")\n",
    "    print(\"Files saved:\")\n",
    "    print(\"- X_train.csv, X_test.csv\")\n",
    "    print(\"- y_train.csv, y_test.csv\")\n",
    "    print(\"- customer_churn_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering summary\n",
    "print(\"=== FEATURE ENGINEERING SUMMARY ===\")\n",
    "print(f\"Original features: {df.shape[1]}\")\n",
    "print(f\"Final features: {X_scaled.shape[1]}\")\n",
    "print(f\"Features added: {X_scaled.shape[1] - df.shape[1]}\")\n",
    "print(f\"\\nFeature types:\")\n",
    "print(f\"- Numerical: {len(numerical_features)}\")\n",
    "print(f\"- Categorical (encoded): {X_scaled.shape[1] - len(numerical_features)}\")\n",
    "print(f\"\\nData preprocessing completed successfully! ðŸŽ‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
